import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from random import randrange
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import xgboost as xgb
from sklearn.metrics import roc_auc_score, roc_curve
# PART A-1
# 1. read the data
data_train = pd.read_csv('data_train.csv')
data_test = pd.read_csv('data_test.csv')

# 2. print the training data
print(data_train.head())
print()
print("Short description of data: ")

# 3. return the shape of the data
shape = data_train.shape
print("Shape of the training data:", shape)

# 4. how many are missing
missing = data_train.isnull().sum()
print("Missing values in each column:", missing)
 # process data to remove missing values
cleaned = data_train.dropna()

# 5. extract features and label from data
features = cleaned.drop(['Loan_Status'], axis=1)
label = cleaned['Loan_Status']

# 6. plot the histograms
numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']
for feature in numerical_features:
    plt.figure(figsize=(5, 2))
    cleaned[feature].hist(bins=20)
    plt.title("Histogram of " + feature)
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.grid(False);
    plt.show()

# 7. feature types
types = cleaned.dtypes
print("Feature types:", types)

numerical_cols = cleaned.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = cleaned.select_dtypes(include=['object', 'category']).columns.tolist()

# The histograms for the features in the loan dataset contain a combination of continuous and categorical variables. Continuous variables
# (such as ApplicantIncome, CoapplicantIncome, and LoanAmount) show a wide range of values, leaning towards lower amounts. This indicates that most 
# applicants have lower incomes and request smaller loan amounts. 

# Categorical variables, represented by binary or limited unique values, include Credit_History, Education, Gender, Married, Self_Employed, 
# and Property_Area. This information reveals demographic and personal information about the applicants, which could influence loan approval decisions.
# PART A-2
# recursively split the tarining data into homogenous groups until the stopping criteria is met. 
# Once met, predictions are made based on the majority class in each terminal node

def giniCalculate(groups, classes):
    num = float(sum([len(group) for group in groups]))     # calculate the total number of instances across all the groups
    gini = 0.0
    for group in groups:
        size = float(len(group))     # size of the current group
        if size == 0:
            continue
        score = 0.0
        for val in classes:
            prop = [row[-1] for row in group].count(val) / size      # proportion of each class in the group
            score += prop * prop       # add the square of proportion to the score
        gini += (1.0 - score) * (size / num)     # weigh the group's score by its size and subtract from 1
    return gini     # return the gini index across all groups

# group1 = [[1, 1], [1, 0]]
# group2 = [[0, 1], [0, 0]]
# groups = [group1, group2]
# classes = [0, 2] 

# index = gini(groups, classes)
# print('Gini index of the split:', index)

# returns the most common output value in the group i.e. prediction
def getPrediction(group):
    for row in group:
        outcome = row[-1]        # extract the last column for all rows in the group
    return max(set(outcome), key=outcome.count)        # return whatever appears the most

# recursively split node into left and right children
def split(node, max_depth, min_size, depth, features):
    left, right = node['groups']      # extracting groups to be split further
    
    del(node['groups'])      # remove for cleanup
    
    if not left or not right:
        node['left'] = getPrediction(left + right)        # split is empty so terminal node
        node['right'] = getPrediction(left + right)        # split is empty so terminal node
        return
    
    # max depth reached
    if depth >= max_depth: 
        node['left'] = getPrediction(left)       # split is empty so terminal node
        node['right'] = getPrediction(right)     # split is empty so terminal node
        return
    
    # process left child
    if len(left) <= min_size:
        node['left'] = getPrediction(left)            # terminal node
    else:    # not a terminal node so can be split further
        node['left'] = findBest(left, features)     # not terminal node
        depth = depth + 1
        split(node['left'], max_depth, min_size, depth, features)
        
    # process right child
    if len(right) <= min_size:        
        node['right'] = getPrediction(right)             # terminal node
    else:      # not a terminal code so can be split further
        node['right'] = findBest(right, features)         # not terminal node
        depth = depth + 1
        split(node['right'], max_depth, min_size, depth, features)

def build_tree(train, max_depth, min_size, features):    # build the tree
    root = findBest(train, features)       # start with root and keep splitting from there
    split(root, max_depth, min_size, 1, features)
    return root

def findBest(data, features):
    class_values = list(set(row[-1] for row in data))
    best_index =  999
    best_value = 999
    best_score = 999
    best_groups = None
    
    for index in range(features):     # for each feature
        for row in data:
            groups = test_split(index, row[index], data)    # create groups based on the split
            gini = giniCalculate(groups, class_values)
            if gini < best_score:
                best_score = gini
                best_index = index
                best_value = row[index]
                best_groups = groups
    return {'index':best_index, 'value':best_value, 'groups':best_groups}

def test_split(index, value, data):
    left = list()
    right = list()
    
    for row in data:   # spli into right group and left grou[
        if row[index] <= value:     # if less, go to left and add a row
            left.append(row)
        else:
            right.append(row)          # go to rigth and add a row
    return left, right

def prediction(node, row):           # recursively traverse the tree until a terminal node is reached
    if row[node['index']] < node['value']:      # check if feature value of the row is less than split value
        if isinstance(node['left'], dict): # check if left child is another sub tree
            return prediction(node['left'], row)
        else:
            return node['left']     # return prediction value of terminal node
    else:
        if isinstance(node['right'], dict):   # check if right child is another sub tree
            return prediction(node['right'], row)
        else:
            return node['right']    # return prediction value of terminal node

X = cleaned.drop(categorical_cols, axis=1)        # get rid of columns without number values
y = cleaned['Loan_Status'].map({'N': 0, 'Y': 1})

num = len(numerical_cols)
train = cleaned.values.tolist()
tree = build_tree(train, max_depth = 3, min_size = 1, features = num)
pred = [prediction(tree, train[0])]

predDF = pd.DataFrame(pred, columns=['pred_tree'])
predDF.to_csv("Manisha_Subrahmanya_preds.csv")

print("Saved to csv file!")
# PART B
X = cleaned.drop(categorical_cols, axis=1)
y = cleaned['Loan_Status'].map({'N': 0, 'Y': 1})

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)       # split dataset into 50% training and test data

def train_XGBoost(X_train, y_train): 
    lambdas = [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3]    # list of lambda values
    best_score = 0        # initialize best
    best_model = None
    best_lambda = None
    kf = KFold(n_splits=10, shuffle=True, random_state=42)     # 10 fold cross validation
    
    for lamb in lambdas:
        aucListScores = []
        for train_index, test_index in kf.split(X_train):
            # split data into training and validation sets for the cuurent fold
            X_train_cross = X_train.iloc[train_index]
            X_test_cross = X_train.iloc[test_index]
            y_train_cross = y_train.iloc[train_index]
            y_test_cross = y_train.iloc[test_index]

            # preparing data for boost
            dTrain = xgb.DMatrix(X_train_cross, label=y_train_cross)
            dTest = xgb.DMatrix(X_test_cross, label=y_test_cross)
            
            params = {"reg_lambda": lamb, "objective": "binary:logistic"}
            best = xgb.train(params, dTrain, num_boost_round=100)    # training the model
            predicts = best.predict(dTest)       # predicting on the validation set
            
            auc = roc_auc_score(y_test_cross, predicts)      # calculate auc score
            aucListScores.append(auc)
        
        aucMean = np.mean(aucListScores)
        if aucMean > best_score:       # updating best values
            best_score = aucMean
            best_model = best
            best_lambda = lamb
    
    print("Best Lambda: ", best_lambda)
    print("AUC: ", best_score)
    
    return best_model

# train the model with the best lambda value
best_model = train_XGBoost(X_train, y_train)

# preparing the test set for ROC curve plotting
dTest = xgb.DMatrix(X_test)
y_pred = best_model.predict(dTest)

fpr, tpr, threshold = roc_curve(y_test, y_pred)
auc_score = roc_auc_score(y_test, y_pred)

# Plotting the ROC curve
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f'XGBoost ROC curve (area = {auc_score:.2f})', color='pink', lw=2)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

predictionsDF = pd.DataFrame(y_pred, columns=['pred_xgboost'])
predictionsDF.to_csv('predictions.csv', index=False)

print("Saved to csv file!")
# The XGBoost performed better. This could be due to many reasons, including reduced
# overfitting. By focusing on correcting the mistakes of previous models, boosting can
# improve the accuracy of the predictions. A single decision tree might be weak and prone to
# errors while boositing combines multiple trees to compensate for the individual weekness
# of a tree. This improves overall performance.
